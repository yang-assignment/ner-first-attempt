{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72a61833",
   "metadata": {},
   "source": [
    "## 1 Load WNUT 17 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feecfac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "\n",
    "def read_wnut(file_path):\n",
    "    file_path = Path(file_path)\n",
    "\n",
    "    raw_text = file_path.read_text().strip()  # 去除末尾（开头）的多余换行符\n",
    "    raw_docs = re.split(r'\\n\\t?\\n', raw_text)  # 以文档单位构成列表，用两个换行（可能中间有制表符）标识\n",
    "    token_docs = []  # 元素为一个文档的所有token\n",
    "    tag_docs = []\n",
    "    for doc in raw_docs:\n",
    "        tokens = []\n",
    "        tags = []\n",
    "        for line in doc.split('\\n'):\n",
    "            token, tag = line.split('\\t')  # 一行中包含token和tag（用制表符分隔）\n",
    "            tokens.append(token)\n",
    "            tags.append(tag)\n",
    "        token_docs.append(tokens)\n",
    "        tag_docs.append(tags)\n",
    "\n",
    "    return token_docs, tag_docs\n",
    "\n",
    "\n",
    "texts, tags = read_wnut('wnut17train.conll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8e8e8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['for', 'two', 'weeks', '.', 'Empire', 'State', 'Building']\n",
      "['O', 'O', 'O', 'O', 'B-location', 'I-location', 'I-location']\n"
     ]
    }
   ],
   "source": [
    "print(texts[0][10:17], tags[0][10:17], sep='\\n')  # 第一个文档的部分token和tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80ec5c5",
   "metadata": {},
   "source": [
    "## 2 Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ac89f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_texts, val_texts, train_tags, val_tags = train_test_split(\n",
    "    texts, tags, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "434a5010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 映射\n",
    "unique_tags = set(tag for doc in tags for tag in doc)  # 有多少种tag并按序排号\n",
    "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()} # 标签用数字表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45159aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对token编码/分词 便于输入到DistllBert\n",
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')\n",
    "train_encodings = tokenizer(train_texts, is_split_into_words=True,\n",
    "                            return_offsets_mapping=True, padding=True, truncation=True)\n",
    "val_encodings = tokenizer(val_texts, is_split_into_words=True,\n",
    "                          return_offsets_mapping=True, padding=True, truncation=True)\n",
    "# 文件包含input_ids，及编码，和offset_mapping，及子标签相对源标签的偏移\n",
    "# 分词后还是[[文档1], [token1编码，token2编码], [token1子token编码，token1子token编码], ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7124c832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['next', 'few', 'weeks', 'are', 'going', 'to', 'be', 'hectic', 'between', 'traveling', 'and', 'knee', 'surgery', 'when', 'i', 'get', 'back']\n",
      "['[CLS]', 'next', 'few', 'weeks', 'are', 'going', 'to', 'be', 'he', '##ctic', 'between', 'traveling', 'and', 'knee', 'surgery', 'when', 'i', 'get', 'back', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "[101, 1397, 1374, 2277, 1132, 1280, 1106, 1129, 1119, 11143, 1206, 6934, 1105, 5656, 6059, 1165, 178, 1243, 1171, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# 原有的词语\n",
    "print(train_texts[0])\n",
    "# 将编码还原成token\n",
    "tokens = tokenizer.convert_ids_to_tokens(train_encodings.input_ids[0])\n",
    "print(tokens)\n",
    "print(train_encodings.input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1426515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# 分词器将一个标记拆分为多个子标记，那么最终会出现标记和标签之间的不匹配\n",
    "\n",
    "\n",
    "def encode_tags(tags, encodings):\n",
    "    labels = [[tag2id[tag] for tag in doc]\n",
    "              for doc in tags]  # 把tags转换映射id（未分词前）\n",
    "    encoded_labels = []\n",
    "    # offset_mapping指示子标记相对于从其拆分的原始标记的开始位置和结束位置。\n",
    "    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):\n",
    "        # create an empty array of -100\n",
    "        doc_enc_labels = np.ones(\n",
    "            len(doc_offset), dtype=int) * -100  # 长度以子标记数量为准\n",
    "        arr_offset = np.array(doc_offset)\n",
    "\n",
    "        # 将那些偏移首位为0，第二位不为零的子token，说明是原来token的第一个子token\n",
    "        # 筛选下来数量就是分词器的token数，标记为对应的id，其他还是-100\n",
    "        doc_enc_labels[(arr_offset[:, 0] == 0) & (\n",
    "            arr_offset[:, 1] != 0)] = doc_labels\n",
    "        encoded_labels.append(doc_enc_labels.tolist())\n",
    "\n",
    "    return encoded_labels\n",
    "\n",
    "\n",
    "train_labels = encode_tags(train_tags, train_encodings)\n",
    "val_labels = encode_tags(val_tags, val_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e7719f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-100,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " -100,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c5f487",
   "metadata": {},
   "source": [
    "## 3 Create a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fab9c81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class WNUTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx])\n",
    "                for key, val in self.encodings.items()} # items就是键、值数组\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "# we don't want to pass this to the model, 仅保留input_ids，即分词编码\n",
    "train_encodings.pop(\"offset_mapping\")\n",
    "val_encodings.pop(\"offset_mapping\")\n",
    "train_dataset = WNUTDataset(train_encodings, train_labels)\n",
    "val_dataset = WNUTDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b5e1f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,  1397,  1374,  2277,  1132,  1280,  1106,  1129,  1119, 11143,\n",
       "          1206,  6934,  1105,  5656,  6059,  1165,   178,  1243,  1171,   102,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor([-100,    0,    0,    0,    0,    0,    0,    0,    0, -100,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f2e62e",
   "metadata": {},
   "source": [
    "## 4 Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a78d8df9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (Temp/ipykernel_7416/1070711664.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\91342\\AppData\\Local\\Temp/ipykernel_7416/1070711664.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    from transformers import DistilBertForTokenClassification，TrainingArguments, Trainer\u001b[0m\n\u001b[1;37m                                                                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForTokenClassification，TrainingArguments, Trainer\n",
    "model = DistilBertForTokenClassification.from_pretrained(\n",
    "    'distilbert-base-cased', num_labels=len(unique_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17690598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数设置\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fa3dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=WNUTDataset(train_encodings, train_labels)\n",
    "    eval_dataset=tokenized_wnut[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
